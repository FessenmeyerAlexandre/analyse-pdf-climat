import os
import uuid
from dotenv import load_dotenv
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# ğŸ” 1. Charger la clÃ© API
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ ClÃ© API OpenAI manquante ou vide. Merci de lâ€™ajouter dans le fichier `.env`.")
    st.stop()
os.environ["OPENAI_API_KEY"] = api_key

# âš™ï¸ 2. Configurer le LLM
Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
Settings.embed_model = OpenAIEmbedding()

# ğŸ¨ 3. UI
st.set_page_config(page_title="Analyse Climat PDF", page_icon="ğŸŒ±", layout="wide")
st.title("ğŸ“„ Analyse du plan de transition climatique")
st.write("DÃ©posez un rapport PDF pour poser jusquâ€™Ã  **40 questions** sur sa stratÃ©gie climat.")

# ğŸ“‚ 4. Upload PDF
uploaded_file = st.file_uploader("ğŸ“ DÃ©posez un fichier PDF ici", type=["pdf"])
if uploaded_file:
    filename = f"{uuid.uuid4().hex}.pdf"
    with open(filename, "wb") as f:
        f.write(uploaded_file.read())

    try:
        with st.spinner("ğŸ“š Lecture et indexation du document..."):
            documents = SimpleDirectoryReader(input_files=[filename]).load_data()
            index = VectorStoreIndex.from_documents(documents)
            query_engine = index.as_query_engine(similarity_top_k=4)
    except Exception as e:
        st.error(f"âŒ Erreur lors de la lecture du fichier : {e}")
        st.stop()

    # â“ Questions utilisateur
    st.subheader("ğŸ§  Posez vos questions (max 40)")
    questions = []
    for i in range(40):
        q = st.text_input(f"Question {i+1}", key=f"q{i}")
        if q.strip():
            questions.append(q.strip())

    if st.button("ğŸ¯ Lancer lâ€™analyse") and questions:
        st.subheader("ğŸ“¬ RÃ©sultats")
        responses = []
        for i, question in enumerate(questions):
            with st.spinner(f"Traitement de la question {i+1}..."):
                try:
                    response = query_engine.query(question)
                    st.markdown(f"**Q{i+1} : {question}**")
                    st.markdown(f"âœ… **RÃ©ponse** : {response.response}")
                    st.markdown("---")
                    responses.append((question, response.response))
                except Exception as err:
                    st.warning(f"Erreur pour la question {i+1} : {err}")

        # ğŸ’¾ TÃ©lÃ©charger les rÃ©sultats
        if responses:
            result_text = "\n\n".join([f"Q{i+1}: {q}\nRÃ©ponse: {r}" for i, (q, r) in enumerate(responses)])
            st.download_button("ğŸ“¥ TÃ©lÃ©charger les rÃ©ponses", result_text.encode(), file_name="rÃ©sultats.txt")
else:
    st.info("â¬†ï¸ Merci de dÃ©poser un fichier PDF pour dÃ©marrer lâ€™analyse.")
